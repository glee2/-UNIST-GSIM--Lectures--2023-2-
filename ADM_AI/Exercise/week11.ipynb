{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BAT512] Advanced Data Mining with AI <br/><br/> 11주차(9강) 실습자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.091722Z",
     "start_time": "2023-11-10T19:57:20.482434Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 로드\n",
    "IMDB 영화 리뷰 데이터\n",
    "    - review: 영화 리뷰 텍스트\n",
    "    - sentiment: 해당 리뷰의 감정 분류(positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.129020Z",
     "start_time": "2023-11-10T19:57:25.096002Z"
    }
   },
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv(\"data/IMDB Dataset.csv\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.262640Z",
     "start_time": "2023-11-10T19:57:25.131519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Nothing is sacred. Just ask Ernie Fosselius. T...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I hated it. I hate self-aware pretentious inan...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I usually try to be professional and construct...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>If you like me is going to see this in a film ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This is like a zoology textbook, given that it...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    One of the other reviewers has mentioned that ...  positive\n",
       "1    A wonderful little production. <br /><br />The...  positive\n",
       "2    I thought this was a wonderful way to spend ti...  positive\n",
       "3    Basically there's a family where a little boy ...  negative\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "995  Nothing is sacred. Just ask Ernie Fosselius. T...  positive\n",
       "996  I hated it. I hate self-aware pretentious inan...  negative\n",
       "997  I usually try to be professional and construct...  negative\n",
       "998  If you like me is going to see this in a film ...  negative\n",
       "999  This is like a zoology textbook, given that it...  negative\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.360451Z",
     "start_time": "2023-11-10T19:57:25.265286Z"
    }
   },
   "outputs": [],
   "source": [
    "X = rawdata[\"review\"]\n",
    "y = rawdata[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html 태그 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.475328Z",
     "start_time": "2023-11-10T19:57:25.366255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (and surprisingly tame) Fulci giallo which means it's still quite bad by normal standards, but redeemed by its solid build-up and some nice touches such as a neat time twist on the issues of visions and clairvoyance.<br /><br />The genre's well-known weaknesses are in full gear: banal dialogue, wooden acting, illogical plot points. And the finale goes on much too long, while the denouement proves to be a rather lame or shall I say: limp affair.<br /><br />Fulci's ironic handling of giallo norms is amusing, though. Yellow clues wherever you look.<br /><br />3 out of 10 limping killers\n"
     ]
    }
   ],
   "source": [
    "print(X[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.574848Z",
     "start_time": "2023-11-10T19:57:25.479342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (and surprisingly tame) Fulci giallo which means it's still quite bad by normal standards, but redeemed by its solid build-up and some nice touches such as a neat time twist on the issues of visions and clairvoyance.The genre's well-known weaknesses are in full gear: banal dialogue, wooden acting, illogical plot points. And the finale goes on much too long, while the denouement proves to be a rather lame or shall I say: limp affair.Fulci's ironic handling of giallo norms is amusing, though. Yellow clues wherever you look.3 out of 10 limping killers\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "html_cleaner = re.compile(\"<.*?>\")\n",
    "print(re.sub(html_cleaner, \"\", X[49]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.699368Z",
     "start_time": "2023-11-10T19:57:25.578840Z"
    }
   },
   "outputs": [],
   "source": [
    "X_html_cleaned = X.apply(lambda x: re.sub(html_cleaner, \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.815201Z",
     "start_time": "2023-11-10T19:57:25.702880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (and surprisingly tame) Fulci giallo which means it's still quite bad by normal standards, but redeemed by its solid build-up and some nice touches such as a neat time twist on the issues of visions and clairvoyance.The genre's well-known weaknesses are in full gear: banal dialogue, wooden acting, illogical plot points. And the finale goes on much too long, while the denouement proves to be a rather lame or shall I say: limp affair.Fulci's ironic handling of giallo norms is amusing, though. Yellow clues wherever you look.3 out of 10 limping killers\n"
     ]
    }
   ],
   "source": [
    "print(X_html_cleaned[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:25.965114Z",
     "start_time": "2023-11-10T19:57:25.819554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average (and surprisingly tame) fulci giallo which means it's still quite bad by normal standards, but redeemed by its solid build-up and some nice touches such as a neat time twist on the issues of visions and clairvoyance.the genre's well-known weaknesses are in full gear: banal dialogue, wooden acting, illogical plot points. and the finale goes on much too long, while the denouement proves to be a rather lame or shall i say: limp affair.fulci's ironic handling of giallo norms is amusing, though. yellow clues wherever you look.3 out of 10 limping killers\n"
     ]
    }
   ],
   "source": [
    "print(X_html_cleaned[49].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:26.109410Z",
     "start_time": "2023-11-10T19:57:25.969246Z"
    }
   },
   "outputs": [],
   "source": [
    "X_lowered = X_html_cleaned.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구두점(punctuation) 및 숫자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:26.223474Z",
     "start_time": "2023-11-10T19:57:26.112764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average (and surprisingly tame) fulci giallo which means it's still quite bad by normal standards, but redeemed by its solid build-up and some nice touches such as a neat time twist on the issues of visions and clairvoyance.the genre's well-known weaknesses are in full gear: banal dialogue, wooden acting, illogical plot points. and the finale goes on much too long, while the denouement proves to be a rather lame or shall i say: limp affair.fulci's ironic handling of giallo norms is amusing, though. yellow clues wherever you look.3 out of 10 limping killers\n"
     ]
    }
   ],
   "source": [
    "print(X_lowered[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:26.325317Z",
     "start_time": "2023-11-10T19:57:26.226898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average and surprisingly tame fulci giallo which means its still quite bad by normal standards but redeemed by its solid buildup and some nice touches such as a neat time twist on the issues of visions and clairvoyancethe genres wellknown weaknesses are in full gear banal dialogue wooden acting illogical plot points and the finale goes on much too long while the denouement proves to be a rather lame or shall i say limp affairfulcis ironic handling of giallo norms is amusing though yellow clues wherever you look out of  limping killers\n"
     ]
    }
   ],
   "source": [
    "punctuation_number_cleaner = re.compile(\"[^a-zA-Z\\ ]\")\n",
    "print(re.sub(punctuation_number_cleaner, \"\", X_lowered[49]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:26.480492Z",
     "start_time": "2023-11-10T19:57:26.329183Z"
    }
   },
   "outputs": [],
   "source": [
    "X_punctuation_number_cleaned = X_lowered.apply(lambda x: re.sub(punctuation_number_cleaner, \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:28.341445Z",
     "start_time": "2023-11-10T19:57:26.487757Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/glee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:28.351830Z",
     "start_time": "2023-11-10T19:57:28.345342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average and surprisingly tame fulci giallo which means its still quite bad by normal standards but redeemed by its solid buildup and some nice touches such as a neat time twist on the issues of visions and clairvoyancethe genres wellknown weaknesses are in full gear banal dialogue wooden acting illogical plot points and the finale goes on much too long while the denouement proves to be a rather lame or shall i say limp affairfulcis ironic handling of giallo norms is amusing though yellow clues wherever you look out of  limping killers\n"
     ]
    }
   ],
   "source": [
    "print(X_punctuation_number_cleaned[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:28.557410Z",
     "start_time": "2023-11-10T19:57:28.354543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average', 'and', 'surprisingly', 'tame', 'fulci', 'giallo', 'which', 'means', 'its', 'still', 'quite', 'bad', 'by', 'normal', 'standards', 'but', 'redeemed', 'by', 'its', 'solid', 'buildup', 'and', 'some', 'nice', 'touches', 'such', 'as', 'a', 'neat', 'time', 'twist', 'on', 'the', 'issues', 'of', 'visions', 'and', 'clairvoyancethe', 'genres', 'wellknown', 'weaknesses', 'are', 'in', 'full', 'gear', 'banal', 'dialogue', 'wooden', 'acting', 'illogical', 'plot', 'points', 'and', 'the', 'finale', 'goes', 'on', 'much', 'too', 'long', 'while', 'the', 'denouement', 'proves', 'to', 'be', 'a', 'rather', 'lame', 'or', 'shall', 'i', 'say', 'limp', 'affairfulcis', 'ironic', 'handling', 'of', 'giallo', 'norms', 'is', 'amusing', 'though', 'yellow', 'clues', 'wherever', 'you', 'look', 'out', 'of', 'limping', 'killers']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(X_punctuation_number_cleaned[49]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:29.516810Z",
     "start_time": "2023-11-10T19:57:28.560430Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tokenized = X_punctuation_number_cleaned.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:29.528459Z",
     "start_time": "2023-11-10T19:57:29.519294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [one, of, the, other, reviewers, has, mentione...\n",
       "1    [a, wonderful, little, production, the, filmin...\n",
       "2    [i, thought, this, was, a, wonderful, way, to,...\n",
       "3    [basically, theres, a, family, where, a, littl...\n",
       "4    [petter, matteis, love, in, the, time, of, mon...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어(stopwords) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:29.720087Z",
     "start_time": "2023-11-10T19:57:29.530786Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_eng = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:29.801033Z",
     "start_time": "2023-11-10T19:57:29.724015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average', 'and', 'surprisingly', 'tame', 'fulci', 'giallo', 'which', 'means', 'its', 'still', 'quite', 'bad', 'by', 'normal', 'standards', 'but', 'redeemed', 'by', 'its', 'solid', 'buildup', 'and', 'some', 'nice', 'touches', 'such', 'as', 'a', 'neat', 'time', 'twist', 'on', 'the', 'issues', 'of', 'visions', 'and', 'clairvoyancethe', 'genres', 'wellknown', 'weaknesses', 'are', 'in', 'full', 'gear', 'banal', 'dialogue', 'wooden', 'acting', 'illogical', 'plot', 'points', 'and', 'the', 'finale', 'goes', 'on', 'much', 'too', 'long', 'while', 'the', 'denouement', 'proves', 'to', 'be', 'a', 'rather', 'lame', 'or', 'shall', 'i', 'say', 'limp', 'affairfulcis', 'ironic', 'handling', 'of', 'giallo', 'norms', 'is', 'amusing', 'though', 'yellow', 'clues', 'wherever', 'you', 'look', 'out', 'of', 'limping', 'killers']\n"
     ]
    }
   ],
   "source": [
    "print(X_tokenized[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:29.916822Z",
     "start_time": "2023-11-10T19:57:29.805050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'original', 'gut', 'wrenching', 'laughter', 'like', 'movie', 'young', 'old', 'love', 'movie', 'hell', 'even', 'mom', 'liked', 'itgreat', 'camp']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in X_tokenized[9] if word not in stopwords_eng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:30.381144Z",
     "start_time": "2023-11-10T19:57:29.920714Z"
    }
   },
   "outputs": [],
   "source": [
    "X_stopwords_cleaned = X_tokenized.apply(lambda x: [word for word in x if word not in stopwords_eng])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩: 시퀀스 길이 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:30.394188Z",
     "start_time": "2023-11-10T19:57:30.383881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      167\n",
       "1       84\n",
       "2       85\n",
       "3       66\n",
       "4      125\n",
       "      ... \n",
       "995    107\n",
       "996     39\n",
       "997    152\n",
       "998    106\n",
       "999    312\n",
       "Name: review, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stopwords_cleaned.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:30.560572Z",
     "start_time": "2023-11-10T19:57:30.397327Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean      118.777000\n",
       "std        87.078838\n",
       "min         8.000000\n",
       "25%        64.000000\n",
       "50%        89.500000\n",
       "75%       146.250000\n",
       "max       621.000000\n",
       "Name: review, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stopwords_cleaned.apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:30.683513Z",
     "start_time": "2023-11-10T19:57:30.564201Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:30.830798Z",
     "start_time": "2023-11-10T19:57:30.687567Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['average',\n",
       " 'surprisingly',\n",
       " 'tame',\n",
       " 'fulci',\n",
       " 'giallo',\n",
       " 'means',\n",
       " 'still',\n",
       " 'quite',\n",
       " 'bad',\n",
       " 'normal',\n",
       " 'standards',\n",
       " 'redeemed',\n",
       " 'solid',\n",
       " 'buildup',\n",
       " 'nice',\n",
       " 'touches',\n",
       " 'neat',\n",
       " 'time',\n",
       " 'twist',\n",
       " 'issues',\n",
       " 'visions',\n",
       " 'clairvoyancethe',\n",
       " 'genres',\n",
       " 'wellknown',\n",
       " 'weaknesses',\n",
       " 'full',\n",
       " 'gear',\n",
       " 'banal',\n",
       " 'dialogue',\n",
       " 'wooden',\n",
       " 'acting',\n",
       " 'illogical',\n",
       " 'plot',\n",
       " 'points',\n",
       " 'finale',\n",
       " 'goes',\n",
       " 'much',\n",
       " 'long',\n",
       " 'denouement',\n",
       " 'proves',\n",
       " 'rather',\n",
       " 'lame',\n",
       " 'shall',\n",
       " 'say',\n",
       " 'limp',\n",
       " 'affairfulcis',\n",
       " 'ironic',\n",
       " 'handling',\n",
       " 'giallo',\n",
       " 'norms',\n",
       " 'amusing',\n",
       " 'though',\n",
       " 'yellow',\n",
       " 'clues',\n",
       " 'wherever',\n",
       " 'look',\n",
       " 'limping',\n",
       " 'killers',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stopwords_cleaned[49]+[\"<PAD>\"]*(MAX_LEN-len(X_stopwords_cleaned[49]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:30.975349Z",
     "start_time": "2023-11-10T19:57:30.835633Z"
    }
   },
   "outputs": [],
   "source": [
    "X_padded = X_stopwords_cleaned.apply(lambda x: x+[\"<PAD>\"]*(MAX_LEN-len(x)) if len(x)<MAX_LEN else x[:MAX_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:31.119856Z",
     "start_time": "2023-11-10T19:57:30.979306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.0\n",
       "mean      100.0\n",
       "std         0.0\n",
       "min       100.0\n",
       "25%       100.0\n",
       "50%       100.0\n",
       "75%       100.0\n",
       "max       100.0\n",
       "Name: review, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded.apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 집합(Vocabulary) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:31.477628Z",
     "start_time": "2023-11-10T19:57:31.123413Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = np.unique(np.concatenate(X_padded.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:31.491515Z",
     "start_time": "2023-11-10T19:57:31.483136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<PAD>', 'aaargh', 'aamir', ..., 'zulu', 'zwick',\n",
       "       'zzzzzzzzzzzzzzzzzz'], dtype='<U56')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:31.675440Z",
     "start_time": "2023-11-10T19:57:31.495785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16277\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:31.861208Z",
     "start_time": "2023-11-10T19:57:31.680201Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:31.973909Z",
     "start_time": "2023-11-10T19:57:31.864780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> 0\n",
      "aaargh 1\n",
      "aamir 2\n",
      "aaron 3\n",
      "ab 4\n",
      "abandon 5\n",
      "abandoned 6\n",
      "abandons 7\n",
      "abba 8\n",
      "abbey 9\n",
      "abbot 10\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(vocab.items()):\n",
    "    print(k, vocab[k])\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:32.095052Z",
     "start_time": "2023-11-10T19:57:31.978165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average', 'surprisingly', 'tame', 'fulci', 'giallo', 'means', 'still', 'quite', 'bad', 'normal', 'standards', 'redeemed', 'solid', 'buildup', 'nice', 'touches', 'neat', 'time', 'twist', 'issues', 'visions', 'clairvoyancethe', 'genres', 'wellknown', 'weaknesses', 'full', 'gear', 'banal', 'dialogue', 'wooden', 'acting', 'illogical', 'plot', 'points', 'finale', 'goes', 'much', 'long', 'denouement', 'proves', 'rather', 'lame', 'shall', 'say', 'limp', 'affairfulcis', 'ironic', 'handling', 'giallo', 'norms', 'amusing', 'though', 'yellow', 'clues', 'wherever', 'look', 'limping', 'killers', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(X_padded[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:32.218505Z",
     "start_time": "2023-11-10T19:57:32.099669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[971, 14078, 14229, 5682, 5903, 8926, 13715, 11397, 1049, 9823, 13598, 11637, 13326, 1863, 9727, 14770, 9638, 14617, 15018, 7504, 15538, 2517, 5858, 15793, 15744, 5687, 5822, 1101, 3823, 16018, 104, 7042, 10810, 10851, 5313, 6003, 9475, 8443, 3687, 11243, 11504, 8026, 12812, 12433, 8330, 212, 7471, 6358, 5903, 9828, 496, 14529, 16179, 2625, 15842, 8453, 8332, 7881, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[word] for word in X_padded[49]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:32.386966Z",
     "start_time": "2023-11-10T19:57:32.224630Z"
    }
   },
   "outputs": [],
   "source": [
    "X_indexed = X_padded.apply(lambda x: [vocab[word] for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:32.492658Z",
     "start_time": "2023-11-10T19:57:32.389977Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [10050, 11997, 9006, 15708, 10293, 4701, 16209...\n",
       "1    [16006, 8390, 11151, 5290, 14308, 15081, 10027...\n",
       "2    [14532, 16006, 15725, 13472, 14617, 6859, 1400...\n",
       "3    [1167, 14466, 5087, 8390, 1670, 7575, 14498, 1...\n",
       "4    [10622, 8856, 8513, 14617, 9299, 15547, 13882,...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:32.650041Z",
     "start_time": "2023-11-10T19:57:32.496094Z"
    }
   },
   "outputs": [],
   "source": [
    "X_preprocessed = torch.tensor(X_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:51.824780Z",
     "start_time": "2023-11-10T19:57:51.809700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_preprocessed = torch.tensor(label_encoder.fit_transform(y), dtype=torch.float)\n",
    "y_preprocessed = y_preprocessed.unsqueeze(1)\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:52.377145Z",
     "start_time": "2023-11-10T19:57:52.341941Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습/테스트 데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:54.647674Z",
     "start_time": "2023-11-10T19:57:54.625651Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X_preprocessed, y_preprocessed, test_size=0.2, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:55.059568Z",
     "start_time": "2023-11-10T19:57:55.053025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 640, Validation size: 160, Test size: 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(X_train)}, Validation size: {len(X_valid)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커스텀 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:57.067740Z",
     "start_time": "2023-11-10T19:57:57.057419Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_out = self.X[idx]\n",
    "        y_out = self.y[idx]\n",
    "        \n",
    "        return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:57.445162Z",
     "start_time": "2023-11-10T19:57:57.433553Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_set = TextDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_set = TextDataset(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "test_set = TextDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:57.890663Z",
     "start_time": "2023-11-10T19:57:57.876976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:58.307064Z",
     "start_time": "2023-11-10T19:57:58.292792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1370,  5094, 10018,  6834,  9446, 13045,  9087,   261, 10018,  9420,\n",
       "          8623,  8623, 10577,  1370,  5094,  9271,  6834,  9446, 10050,  4883,\n",
       "          3229, 14532,  6138,  3112, 11797, 13766, 11563, 11563,  4643,  9320,\n",
       "          1665, 13770,  6301,  6732,   420, 10050, 11563,  2557,  6255,  6138,\n",
       "          4599, 13766,  6138, 14529,  8309,  9475,  2318,  3569,  3801,  8342,\n",
       "         10758,  6029,  1049,  6834,  1424,  6543,  2600,  5029,   274,  5173,\n",
       "          5709,  1424, 11563,  5709, 12721,  4466,  6146,  8007,  4492, 14766,\n",
       "            37, 16112, 15702, 10405,  9420,  2820, 15695, 14617, 14986, 14491,\n",
       "          8309, 16107,  9420, 11999,  7065,  8431,  5873,  7825, 10018,  4157,\n",
       "          8028,  5889, 14739,  1390, 14337, 13342,   420, 12185,  4953,  6391]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:57:58.689204Z",
     "start_time": "2023-11-10T19:57:58.675611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 모델 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 모델에 대한 PyTorch 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:05.364133Z",
     "start_time": "2023-11-10T19:58:05.338508Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Embedding 계층\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # RNN 계층\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 완전연결층\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text -> (batch_size, MAX_LEN)\n",
    "        \n",
    "        embedded = self.embedding(text) \n",
    "        # embedded -> (batch_size, MAX_LEN, embedding_dim)\n",
    "        \n",
    "        output, hidden_state = self.rnn(embedded)\n",
    "        # output -> (batch_size, MAX_LEN, hidden_dim) 전체 시점의 hidden state\n",
    "        # hidden_state -> (batch_size, 1, hidden_dim) 마지막 시점의 hidden state\n",
    "        \n",
    "        output = torch.sigmoid(self.fc(hidden_state.squeeze()))\n",
    "        # output -> (batch_size, output_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:06.028955Z",
     "start_time": "2023-11-10T19:58:06.021362Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:06.416114Z",
     "start_time": "2023-11-10T19:58:06.402094Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:06.773575Z",
     "start_time": "2023-11-10T19:58:06.766214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(16277, 16)\n",
      "  (rnn): RNN(16, 16, batch_first=True)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:07.122966Z",
     "start_time": "2023-11-10T19:58:07.112124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 파라미터 수:  260993\n"
     ]
    }
   ],
   "source": [
    "print(\"총 파라미터 수: \",sum(p.numel() for p in model.parameters()))\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 수행 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:09.025689Z",
     "start_time": "2023-11-10T19:58:09.016211Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, data_loader, criterion, optimizer, train=False):\n",
    "    loss_ep = 0\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        if train:\n",
    "            y_predicted = model(x)\n",
    "            loss = criterion(y_predicted, y)\n",
    "            loss_ep += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.autograd.no_grad():\n",
    "                y_predicted = model(x)\n",
    "                loss = criterion(y_predicted, y)\n",
    "                loss_ep += loss.item()\n",
    "\n",
    "        return loss_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 및 학습 방식 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:58:40.132937Z",
     "start_time": "2023-11-10T19:58:40.117265Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "max_epochs = 2000\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:06.485389Z",
     "start_time": "2023-11-10T19:58:40.942187Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:100, train_loss: 0.7183, valid_loss: 0.7172\n",
      "epoch:200, train_loss: 0.6816, valid_loss: 0.7171\n",
      "epoch:300, train_loss: 0.7106, valid_loss: 0.7172\n",
      "epoch:400, train_loss: 0.6900, valid_loss: 0.7173\n",
      "epoch:500, train_loss: 0.6793, valid_loss: 0.7173\n",
      "epoch:600, train_loss: 0.6938, valid_loss: 0.7175\n",
      "epoch:700, train_loss: 0.6817, valid_loss: 0.7176\n",
      "epoch:800, train_loss: 0.6882, valid_loss: 0.7176\n",
      "epoch:900, train_loss: 0.6965, valid_loss: 0.7176\n",
      "epoch:1000, train_loss: 0.6720, valid_loss: 0.7177\n",
      "epoch:1100, train_loss: 0.6618, valid_loss: 0.7179\n",
      "epoch:1200, train_loss: 0.6948, valid_loss: 0.7181\n",
      "epoch:1300, train_loss: 0.6804, valid_loss: 0.7183\n",
      "epoch:1400, train_loss: 0.6628, valid_loss: 0.7185\n",
      "epoch:1500, train_loss: 0.6682, valid_loss: 0.7187\n",
      "epoch:1600, train_loss: 0.6382, valid_loss: 0.7188\n",
      "epoch:1700, train_loss: 0.6733, valid_loss: 0.7190\n",
      "epoch:1800, train_loss: 0.6785, valid_loss: 0.7192\n",
      "epoch:1900, train_loss: 0.6712, valid_loss: 0.7194\n",
      "epoch:2000, train_loss: 0.7210, valid_loss: 0.7196\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, max_epochs+1):\n",
    "    train_loss = run_epoch(model, train_loader, criterion, optimizer, train=True)\n",
    "    valid_loss = run_epoch(model, valid_loader, criterion, optimizer, train=False)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"epoch:{}, train_loss: {:.4f}, valid_loss: {:.4f}\".format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:11.945451Z",
     "start_time": "2023-11-10T19:59:11.935418Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:12.394534Z",
     "start_time": "2023-11-10T19:59:12.360625Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): # 학습(가중치 업데이트)을 진행하지 않음\n",
    "    X_test, y_test = next(iter(test_loader))\n",
    "    \n",
    "    y_test_pred = model(X_test)\n",
    "    y_test_pred = torch.round(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:13.143847Z",
     "start_time": "2023-11-10T19:59:13.138054Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test = y_test.detach().numpy()\n",
    "y_test_pred = y_test_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:13.714042Z",
     "start_time": "2023-11-10T19:59:13.694194Z"
    }
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_test_pred, target_names=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:14.367371Z",
     "start_time": "2023-11-10T19:59:14.360864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.15      0.23       108\n",
      "    positive       0.46      0.84      0.59        92\n",
      "\n",
      "    accuracy                           0.47       200\n",
      "   macro avg       0.49      0.49      0.41       200\n",
      "weighted avg       0.49      0.47      0.40       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:16.327289Z",
     "start_time": "2023-11-10T19:59:16.318367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW text: Bela Lugosi appeared in several of these low budget chillers for Monogram Studios in the 1940's and The Corpse Vanishes is one of the better ones.<br /><br />Bela plays a mad scientist who kidnaps young brides and kills them and then extracts fluid from their bodies so he can keep his ageing wife looking young. After a reporter and a doctor stay the night at his home and discover he is responsible for the brides' deaths, the following morning they report these murders to the police and the mad scientist is shot and drops dead shortly afterwards.<br /><br />You have got almost everything in this movie: the scientist's assistants consist of an old hag, a hunchback and dwarf (her sons), a thunderstorm and spooky passages in Bela's house. Bela and his wife find they sleep better in coffins rather than beds in the movie.<br /><br />The Corpse Vanishes is worth a look, especially for Bela Lugosi fans. Great fun.<br /><br />Rating: 3 stars out of 5. \n",
      "\n",
      "SENTIMENT: positive\n"
     ]
    }
   ],
   "source": [
    "sample_index = 52\n",
    "print(\"REVIEW text:\", X[sample_index], \"\\n\\nSENTIMENT:\", y[sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:17.071871Z",
     "start_time": "2023-11-10T19:59:17.059887Z"
    }
   },
   "outputs": [],
   "source": [
    "y_sample_pred = int(torch.round(model(X_preprocessed[sample_index].unsqueeze(0))).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:59:17.604618Z",
     "start_time": "2023-11-10T19:59:17.596257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SENTIMENT: positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted SENTIMENT:\", label_encoder.classes_[y_sample_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
