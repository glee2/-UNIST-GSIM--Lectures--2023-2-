{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BAT512] Advanced Data Mining with AI <br/><br/> 개인과제 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*구글 드라이브 연결과 라이브러리 임포트는 실습자료 참고하여 본인 경로에 맞게 설정하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T15:12:02.811528Z",
     "start_time": "2023-11-17T15:12:02.803933Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:06:39.025340Z",
     "start_time": "2023-11-17T16:06:37.990211Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_container, y_train_container = [], []\n",
    "for i in range(1,6):\n",
    "    curr_data = unpickle(f\"data/cifar-10-batches-py/data_batch_{i}\")\n",
    "    X_train_container.append(curr_data[b'data'])\n",
    "    y_train_container.append(curr_data[b'labels'])\n",
    "\n",
    "X_train = torch.tensor(np.array(X_train_container), dtype=torch.float)\n",
    "y_train = torch.tensor(np.array(y_train_container))\n",
    "\n",
    "X_train = X_train.view(50000, 3072)\n",
    "y_train = y_train.view(50000)\n",
    "\n",
    "test_data = unpickle(f\"data/cifar-10-batches-py/test_batch\")\n",
    "X_test = torch.tensor(np.array(test_data[b'data']), dtype=torch.float)\n",
    "y_test = torch.tensor(np.array(test_data[b'labels']))\n",
    "\n",
    "X_test = X_test.view(10000, 3072)\n",
    "y_test = y_test.view(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:06:39.033134Z",
     "start_time": "2023-11-17T16:06:39.028057Z"
    }
   },
   "outputs": [],
   "source": [
    "# sol\n",
    "X_train = X_train.view(-1, 3, 32, 32)\n",
    "# y_train = nn.functional.one_hot(y_train.view(50000))\n",
    "\n",
    "X_test = X_test.view(-1, 3, 32, 32)\n",
    "# y_test = nn.functional.one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T15:26:25.682978Z",
     "start_time": "2023-11-17T15:26:25.677470Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T15:33:01.862899Z",
     "start_time": "2023-11-17T15:33:01.856434Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:06:41.475239Z",
     "start_time": "2023-11-17T16:06:41.394249Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_set = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T15:35:17.303334Z",
     "start_time": "2023-11-17T15:35:17.296867Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T15:35:17.783198Z",
     "start_time": "2023-11-17T15:35:17.766943Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 첫 번째 계층(Conv1+Pool1)\n",
    "        # 입력 데이터: (?, 32, 32, 1)\n",
    "        #    Conv1 -> (?, 32, 32, 32)\n",
    "        #    MaxPool1 -> (?, 16, 16, 32)\n",
    "        self.layer_1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 두 번째 계층(Conv2+Pool2)\n",
    "        # 입력 데이터: (?, 16, 16, 32)\n",
    "        #    Conv2 -> (?, 16, 16, 64)\n",
    "        #    MaxPool2 -> (?, 8, 8, 64)\n",
    "        self.layer_2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        n_flatten = 8 * 8 * 64 # 일렬로 펴져서 완전연결층에 전달되는 벡터의 크기\n",
    "\n",
    "        # 세 번째 계층(fully-connected)\n",
    "        # 입력 데이터: (?, 8*8*64)\n",
    "        #     fc1 -> (?, 16)\n",
    "        #     fc2 -> (?, 10)\n",
    "        self.FC_layer = nn.Sequential(\n",
    "                nn.Linear(n_flatten, 16),\n",
    "                nn.Linear(16, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.layer_1(x)\n",
    "        hidden = self.layer_2(hidden)\n",
    "        flattened = self.Flatten(hidden) # 일렬로 펴는 작업\n",
    "        out = self.FC_layer(flattened)\n",
    "        return out\n",
    "\n",
    "    def Flatten(self, inputs):\n",
    "        shapes = inputs.size(1) * inputs.size(2) * inputs.size(3)\n",
    "        outputs = inputs.view(inputs.size(0), shapes)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:07:01.504307Z",
     "start_time": "2023-11-17T16:07:01.492388Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:07:03.824444Z",
     "start_time": "2023-11-17T16:07:03.814340Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, data_loader, criterion, optimizer, train=False):\n",
    "    loss_ep = 0\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        if train:\n",
    "            y_predicted = model(x)\n",
    "            loss = criterion(y_predicted, y)\n",
    "            loss_ep += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.autograd.no_grad():\n",
    "                y_predicted = model(x)\n",
    "                loss = criterion(y_predicted, y)\n",
    "                loss_ep += loss.item()\n",
    "\n",
    "        return loss_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:07:04.194130Z",
     "start_time": "2023-11-17T16:07:04.186318Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "max_epochs = 200\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T16:07:39.428082Z",
     "start_time": "2023-11-17T16:07:22.215790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, train_loss: 14.8553\n",
      "epoch:20, train_loss: 11.7579\n",
      "epoch:30, train_loss: 13.7543\n",
      "epoch:40, train_loss: 13.5583\n",
      "epoch:50, train_loss: 23.8720\n",
      "epoch:60, train_loss: 27.2049\n",
      "epoch:70, train_loss: 18.5986\n",
      "epoch:80, train_loss: 25.2318\n",
      "epoch:90, train_loss: 26.0924\n",
      "epoch:100, train_loss: 22.7545\n",
      "epoch:110, train_loss: 15.0922\n",
      "epoch:120, train_loss: 16.3726\n",
      "epoch:130, train_loss: 16.4938\n",
      "epoch:140, train_loss: 20.1297\n",
      "epoch:150, train_loss: 15.2045\n",
      "epoch:160, train_loss: 13.0863\n",
      "epoch:170, train_loss: 12.5833\n",
      "epoch:180, train_loss: 9.6743\n",
      "epoch:190, train_loss: 10.8406\n",
      "epoch:200, train_loss: 12.6670\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, max_epochs+1):\n",
    "    train_loss = run_epoch(model, train_loader, criterion, optimizer, train=True)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch:{}, train_loss: {:.4f}\".format(epoch, train_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
